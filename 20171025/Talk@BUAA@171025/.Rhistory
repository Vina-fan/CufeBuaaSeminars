setwd('~/Documents/Seminars/Talk@Buaa@170612/')
library(slidify)
install.packages('slidify')
library(devtools)
install_github('slidify', 'ramnathv')
install_github('slidifyLibraries', 'ramnathv')
library(slidify)
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
page=1
webpage = sprintf("https://book.douban.com/top250?start=%s", page)
link=webpage
link = as.character(link)
web = read_html(link)
#图书中外文名称
book_name <- web %>%
html_nodes(xpath = '//td/div[@class="pl2"]/a') %>%
html_text()
library(rvest)
link = as.character(link)
web = read_html(link)
#图书中外文名称
book_name <- web %>%
html_nodes(xpath = '//td/div[@class="pl2"]/a') %>%
html_text()
link <- web %>%
html_nodes(xpath = '//td/div[@class="pl2"]/a') %>%
html_attr('href')
book_detail <- web %>%
html_nodes(xpath = '//td/p[@class="pl"]') %>%
html_text()
book_detail
webpage
a=web %>%
html_nodes(xpath = '//p[@class="quote"]') %>%
html_text()
a
a=web %>%
html_nodes(xpath = '//p[@class="quote"]')
a
i=8
page = 25 * (i - 1)
page = as.character(page)
webpage = sprintf("https://book.douban.com/top250?start=%s", page)
webpage
link=webpage
link = as.character(link)
web = read_html(link)
a=web %>%
html_nodes(xpath = '//p[@class="quote"]')
a
for (i in a){i%>%html_nodes(xpath='//span')%>%html_text()}
for (i in a){print(i%>%html_nodes(xpath='//span')%>%html_text())}
i
i%>%html_nodes(xpath='span')
i%>%html_nodes(xpath='span')%>%html_text()
for (i in a){print(i%>%html_nodes(xpath='span')%>%html_text())}
for (i in a){print(length(i%>%html_nodes(xpath='span')%>%html_text())==0)}
for (i in a){print(length(i%>%html_nodes(xpath='span')%>%html_text()))}
for (i in a){print(nchar(i%>%html_nodes(xpath='span')%>%html_text()))}
for (i in a){print(nchar(i%>%html_nodes(xpath='span')))}
for (i in a){print(i%>%html_nodes(xpath='span'))}
length(a)
webpage
a=web %>%
html_nodes(xpath = '//tr[@class="item"]')
length(a)
i=a[1]
i%>%
+     html_nodes(xpath = '//p[@class="quote"]')
i
print i
print(i)
i%>%html_nodes(xpath = '//p[@class="quote"]')
b=i%>%html_nodes(xpath = '//p[@class="quote"]')
b[1]
b[1]%>%html_text()
b[2]%>%html_text()
html_nodes(i,xpath = '//p[@class="quote"]')
?html_nodes
html_node(i,xpath = '//p[@class="quote"]')
html_node(i,xpath = '//p[@class="quote"]') %>%html_text()
i=a[2]
html_node(i,xpath = '//p[@class="quote"]') %>%html_text()
i
a[1]==a[2]
nchar(a[1])
nchar(a[2])
html_node(i,xpath = '//p[@class="quote"]') %>%html_text()
html_nodes(i,xpath = '//p[@class="quote"]') %>%html_text()
html_nodes(i,xpath = '/p[@class="quote"]') %>%html_text()
html_nodes(i,xpath = '/td[2]/p[@class="quote"]') %>%html_text()
html_nodes(i,xpath = '/tr/td[2]/p[@class="quote"]') %>%html_text()
html_nodes(i,xpath = '/tr/td[2]/p[@class="quote"]') %>%html_href()
html_nodes(web,xpath = '/p[@class="quote"]') %>%html_attr('class')
html_nodes(web,xpath = '/p[@class="quote"]')
html_nodes(web,xpath = '//p[@class="quote"]') %>%html_attr('class')
a
a[1]
extractor <- function(x){
worker <- function(x){
ifelse(
is.null(html_node(x, xpath="p")),
NA,
html_node(x, xpath="td[2]/p")  %>%
html_text()
)
}
unlist(lapply(x, worker))
}
a[1]%>%extractor()
extractor <- function(x){
worker <- function(x){
ifelse(
is.null(html_node(x, xpath="p")),
NA,
html_node(x, xpath="td[2]/p[2]")  %>%
html_text()
)
}
unlist(lapply(x, worker))
}
a[1]%>%extractor()
a[2]%>%extractor()
a[3]%>%extractor()
a[4]%>%extractor()
page=1
link = sprintf("https://book.douban.com/top250?start=%s", page)
web %>%
html_nodes(xpath = '//td/p[@class="pl"]') %>%
html_text()
link
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
url='<html><body><p,class='我'>s</p></body></html>'
url='<html><body><p,class="我">s</p></body></html>'
web=read_html(url)
web%>%html_nodes('//p[@class="我"]')%>%html_text()
web%>%html_nodes(xpath='//p[@class="我"]')%>%html_text()
web
url='<html><body><p class="我">s</p></body></html>'
web=read_html(url)
url='<html><body><p class="我">s</p></body></html>'
web%>%html_nodes(xpath='//p[@class="我"]')%>%html_text()
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
a[4]%>%extractor()
a[3]%>%extractor()
quote_extractor <- function(x){
worker <- function(x){
ifelse(
is.null(html_node(x, xpath="p")),
NA,
html_node(x, xpath=".//p[2]")  %>%
html_text()
)
}
unlist(lapply(x, worker))
}
a[3]%>%quote_extractor()
a[3]
quote_extractor <- function(x){
worker <- function(x){
ifelse(
is.null(html_node(x, xpath="p")),
NA,
html_node(x, xpath=".//p[2]")  %>%
html_text()
)
}
unlist(lapply(x, worker))
}
items = web %>% html_nodes(xpath = '//tr[@class="item"]')
items %>% quote_extractor
items %>% quote_extractor()
item
items
webpage = sprintf("https://book.douban.com/top250?start=%s", page)
i=8
page = 25 * (i - 1)
page = as.character(page)
webpage = sprintf("https://book.douban.com/top250?start=%s", page)
webpage
link=webpage
web = read_html(link)
items = web %>% html_nodes(xpath = '//tr[@class="item"]')
items %>% quote_extractor()
quote_extractor <- function(x){
worker <- function(x){
ifelse(
is.null(html_node(x, xpath="p")),
NA,
trimws(html_node(x, xpath=".//p[2]")  %>%
html_text())
)
}
unlist(lapply(x, worker))
}
items = web %>% html_nodes(xpath = '//tr[@class="item"]')
items %>% quote_extractor()
items[1]
items[4]
html_node(x, xpath=".//p")
x=items[4]
html_node(x, xpath=".//p")
html_node(x, xpath="p")
is.null(html_node(x, xpath="p"))
items[4]
link
quote_extractor <- function(x){
worker <- function(x){
ifelse(
is.null(html_node(x, xpath=".//p[2]")),
NA,
trimws(html_node(x, xpath=".//p[2]")  %>%
html_text())
)
}
unlist(lapply(x, worker))
}
items = web %>% html_nodes(xpath = '//tr[@class="item"]')
items %>% quote_extractor()
x=items[4]
x
html_node(x, xpath=".//p[2]")
x=items[3]
html_node(x, xpath=".//p[2]")
worker(x)
quote_extractor(items)
x=items
worker <- function(x){
ifelse(
is.null(html_node(x, xpath=".//p[2]")),
NA,
trimws(html_node(x, xpath=".//p[2]")  %>%
html_text())
)
}
lapply(x, worker)
work(items[4])
worker(items[4])
x=items[4]
is.null(html_node(x, xpath=".//p[2]"))
trimws(html_node(x, xpath=".//p[2]")  %>%
html_text())
html_node(x, xpath=".//p[2]")
quote_extractor <- function(x){
worker <- function(x){
trimws(html_node(x, xpath=".//p[2]")  %>%
html_text())
}
unlist(lapply(x, worker))
}
items = web %>% html_nodes(xpath = '//tr[@class="item"]')
items %>% quote_extractor()
worker <- function(x){
trimws(html_node(x, xpath=".//p[2]")  %>%
html_text())
}
lapply(items, worker)
quote_extractor <- function(x) {
trimws(html_node(x, xpath = ".//p[2]")  %>%
html_text())
}
items = web %>% html_nodes(xpath = '//tr[@class="item"]')
unlist(lapply(items, quote_extractor))
link
web = read_html(link)
items[4]
trimws(html_node(items[3], xpath = ".//p[2]")  %>%
html_text())
trimws(html_node(items[4], xpath = ".//p[2]")  %>%
html_text())
link = 'https://book.douban.com/top250?start=175'
web = read_html(link)
quote_extractor <- function(x) {
trimws(html_nodes(x, xpath = ".//p[2]")  %>%
html_text())
}
items = web %>% html_nodes(xpath = '//tr[@class="item"]')
unlist(lapply(items, quote_extractor))
quote_extractor <- function(x) {
myNode <- html_nodes(x, xpath = ".//p[2]")
ifelse(is.null(myNode), 'NA', trimws(myNode %>% html_text())
}
quote_extractor <- function(x) {
myNode <- html_nodes(x, xpath = ".//p[2]")
ifelse(is.null(myNode), 'NA', trimws(myNode %>% html_text()))
}
items = web %>% html_nodes(xpath = '//tr[@class="item"]')
unlist(lapply(items, quote_extractor))
html_nodes(x, xpath = ".//p[2]")
myNode <- html_nodes(x, xpath = ".//p[2]")
is.null(myNode)
myNode
trimws(myNode %>% html_text())
is.null(myNode)
length(myNode) == 0
quote_extractor <- function(x) {
myNode <- html_nodes(x, xpath = ".//p[2]")
ifelse(length(myNode) == 0, 'NA', trimws(myNode %>% html_text()))
}
items = web %>% html_nodes(xpath = '//tr[@class="item"]')
unlist(lapply(items, quote_extractor))
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
library(rvest)
url <- 'https://www.amazon.cn/s/ref=nb_sb_noss?__mk_zh_CN=%E4%BA%9A%E9%A9%AC%E9%80%8A%E7%BD%91%E7%AB%99&field-keywords=%E5%A4%A7%E6%95%B0%E6%8D%AE'
web <- read_html(url, encoding = "utf-8")
book_nodes<-html_nodes(web,xpath ='//*[@id="result_0"]/div/div/a/h3')
book<-html_text(book_nodes)
book_nodes<-html_nodes(web,xpath ='//*[@id="result_0"]/div/div/a/h3[@data-attribute="平装"]')
book_nodes
html_text(book_nodes)
book_nodes<-html_nodes(web,xpath ='//*[@id="result_0"]/div/div/a/h3[1]')
book<-html_text(book_nodes)
book
html_nodes(web,xpath ='//*[@id="result_0"]/div/div/a/h3')
html_nodes(web,xpath ='//*[@id="result_0"]/div/div/a/h3[1]')
html_nodes(web,xpath ='//*[@id="result_0"]/div/div/a/h3[@data-attribute="平装"]')
html_node(web,xpath ='//*[@id="result_0"]/div/div/a/h3[@data-attribute="平装"]')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
setwd('~/Documents/Seminars/Talk@ecoSta@170616/')
rmarkdown::shiny_prerendered_clean('beamerPresentation.Rmd')
unlink('beamerPresentation_cache', recursive = TRUE)
?nonpar
?spline
?archtest
??arch
datasetTrain <- read.csv("https://yanfei.site/docs/dpsa/train.csv", header=TRUE, sep = ",")
datasetTrain <- read.csv("https://yanfei.site/docs/dpsa/train.csv", header=TRUE, sep = ",")
??read_csv
x=scan('~/Desktop/assignments/marks')
hist(x)
